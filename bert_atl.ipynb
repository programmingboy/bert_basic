{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg1Auv6c42OI",
        "outputId": "76aceb9f-392e-45e5-b798-fbcd51308d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Importing stock ml libraries  (#Requirements)\n",
        "!pip install seqeval\n",
        "!pip install transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from torch import cuda\n",
        "from seqeval.metrics import f1_score, accuracy_score\n",
        "from seqeval.metrics import classification_report\n",
        "from google.colab import drive\n",
        "\n",
        "def requirements():\n",
        "  print(\"numpy \",np.__version__)\n",
        "  print(\"pandas \",pd.__version__)\n",
        "  print(\"sklearn \",sklearn.__version__)\n",
        "  print(\"torch \",torch.__version__)\n",
        "  print(\"transformers\", transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cutbo9lVhw7P"
      },
      "outputs": [],
      "source": [
        "def te_sentiment(rating):\n",
        "  rating = int(rating)\n",
        "  if rating == 2:\n",
        "    return 'I'\n",
        "  elif rating == 1:\n",
        "    return 'B'\n",
        "  else:\n",
        "    return 'O'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2xqPZseglYK"
      },
      "outputs": [],
      "source": [
        "def te_target(rating):\n",
        "  rating = str(rating)\n",
        "  if rating == 'I':\n",
        "    return 3\n",
        "  elif rating == 'B':\n",
        "    return 2\n",
        "  else:\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do-LQDCttQcD"
      },
      "outputs": [],
      "source": [
        "def process_data(df):\n",
        "\n",
        "    df_blank1 = pd.DataFrame()\n",
        "    df_blank1['temp'] = 0\n",
        "    df_blank1['temp'] = df_blank1['temp'].astype('object')\n",
        "    df_blank2 = pd.DataFrame()\n",
        "    df_blank2['target'] = 0\n",
        "    df_blank2['target'] = df_blank2['target'].astype('object')\n",
        "\n",
        "    enc_tag = preprocessing.LabelEncoder() \n",
        "    sentences = df.groupby(\"Sentence\")[\"Word\"].apply(list).values\n",
        "    #print(len(sentences))\n",
        "    counter = 0\n",
        "    for i in sentences:\n",
        "      #print(i)\n",
        "      df_blank1.at[counter, 'temp'] = np.array(i)\n",
        "      counter = counter + 1\n",
        "    tag = df.groupby(\"Sentence\")[\"target\"].apply(list).values\n",
        "    c = 0\n",
        "    for i in tag:\n",
        "      #print(i)\n",
        "      df_blank2.at[c, 'target'] = np.asarray(i)\n",
        "      c = c + 1\n",
        "\n",
        "    df_test = pd.concat([df_blank1,df_blank2], axis=1)\n",
        "    \n",
        "    return sentences, tag, enc_tag, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZOyrQ977bV-"
      },
      "outputs": [],
      "source": [
        "def data_loader(data_test_category, data_test_sentense, data_test_target):\n",
        "  df_test_category = pd.read_csv(data_test_category)\n",
        "  if('aspectCategories' in  df_test_category):\n",
        "    df_test_category = df_test_category.drop(['aspectCategories'], axis=1)\n",
        "  df_test_category = df_test_category.reset_index(drop=True)\n",
        "  df_test_category['list'] = df_test_category[df_test_category.columns[1:]].values.tolist()\n",
        "  df_test_category = df_test_category[['text', 'list']].copy()\n",
        "  print(\"-----------------------------------------------------\")\n",
        "  print(df_test_category.head(5)) \n",
        "\n",
        "  df_test_sentense = pd.read_csv(data_test_sentense, sep='delimiter', names=[\"Sentence1\"])\n",
        "  df_test_target = pd.read_csv(data_test_target, sep='delimiter', names=[\"target1\"])\n",
        "  df_test_combo = pd.concat([df_test_sentense, df_test_target], axis=1)\n",
        "\n",
        "  print(\"-----------------------------------------------------\")\n",
        "  print(df_test_combo.head(5))\n",
        "\n",
        "  u = df_test_combo.Sentence1.str.split(expand=True).stack()\n",
        "  v = df_test_combo.target1.str.split(expand=True).stack()\n",
        "\n",
        "  df_test_combo = pd.DataFrame({\n",
        "        'Sentence': u.index.get_level_values(0) + 1, \n",
        "        'Word': u.values, \n",
        "        'target': v.values,\n",
        "        #'Entity': u.map(dict(zip(df.value, df.entity))).fillna('Object').values\n",
        "    })\n",
        "  print(\"-----------------------------------------------------\")\n",
        "  print(df_test_combo.head(5))\n",
        "\n",
        "  df_test_combo['target'] = df_test_combo.target.apply(te_sentiment)\n",
        "\n",
        "  print(\"-----------------------------------------------------\")\n",
        "  print(df_test_combo.head(5))\n",
        "\n",
        "  df_test_combo['target'] = df_test_combo.target.apply(te_target)\n",
        "\n",
        "  print(\"-----------------------------------------------------\")\n",
        "  print(df_test_combo.head(5))\n",
        "\n",
        "  sentences_te, tag_te, enc_tag_te, df_test = process_data(df_test_combo) \n",
        "\n",
        "  df_test = pd.concat([df_test, df_test_category], axis=1)\n",
        "\n",
        "  \n",
        "  return df_test\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "  \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_DalaqA74Tk"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.temp\n",
        "        \n",
        "        self.tag = self.data.target\n",
        "        self.targets = self.data.list\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        tags = self.tag[index]\n",
        "        targets = self.targets[index]\n",
        "\n",
        "        ids = []\n",
        "        target_tag =[]\n",
        "        #target =[]\n",
        "        MAX_LEN = 100\n",
        "        for i, s in enumerate(text):\n",
        "            inputs = self.tokenizer.encode(\n",
        "                s,\n",
        "                add_special_tokens=False\n",
        "            )\n",
        "           \n",
        "            input_len = len(inputs)\n",
        "            ids.extend(inputs)\n",
        "            target_tag.extend([tags[i]] * input_len)\n",
        "\n",
        "        ids = ids[:MAX_LEN - 2]\n",
        "        target_tag = target_tag[:MAX_LEN - 2]\n",
        "\n",
        "        ids = [101] + ids + [102]\n",
        "        target_tag = [1] + target_tag + [1]\n",
        "\n",
        "        mask = [1] * len(ids)\n",
        "        token_type_ids = [0] * len(ids)\n",
        "\n",
        "        padding_len = MAX_LEN - len(ids)\n",
        "\n",
        "        ids = ids + ([0] * padding_len)\n",
        "        mask = mask + ([0] * padding_len)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
        "        target_tag = target_tag + ([0] * padding_len)\n",
        "\n",
        "        #marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "        #tokenized_text = self.tokenizer.encode(text)\n",
        "        '''\n",
        "        print(text)\n",
        "        print(\"***********************\")\n",
        "        #print(tokenized_text)\n",
        "        #print(\"***********************\")\n",
        "        print(tags)\n",
        "        print(\"***********************\")\n",
        "        print(ids)\n",
        "        print(\"***********************\")\n",
        "        print(mask)\n",
        "        print(\"***********************\")\n",
        "        print(token_type_ids)\n",
        "        print(\"***********************\")\n",
        "        print(target_tag)\n",
        "        print(\"***********************\")\n",
        "        print(targets)\n",
        "        print(\"***********************\")\n",
        "        '''\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDS0CPVsDTab"
      },
      "outputs": [],
      "source": [
        "class BERTClass(torch.nn.Module, ):\n",
        "    def __init__(self, numclasses,numlayers,num_heads, embeddim, hiddendim_lstm, fchidden):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.numclasses = numclasses\n",
        "        self.embeddim = embeddim\n",
        "        self.numlayers = numlayers\n",
        "        self.hiddendim_lstm = hiddendim_lstm\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Dropout(0.3)\n",
        "        self.l4 = torch.nn.Linear(768, 4)\n",
        "        self.l5 = torch.nn.Linear(768, 5)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        o1, pooled_output = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        #print(output_2.shape)\n",
        "        output_2 = self.l2(o1)\n",
        "        #print(output_2.shape)\n",
        "        output_3 = self.l3(pooled_output)\n",
        "        #output_2 = self.lstm(output_2, None)\n",
        "        #output_2 = self.conv(output_2)\n",
        "        #output_2 = self.conv(output_2)\n",
        "        #print(output_2.shape)\n",
        "        output = self.l4(output_2)\n",
        "        \n",
        "        out = self.l5(output_3)\n",
        "        return output, out\n",
        "\n",
        "\n",
        "\n",
        "class Bert_LSTM(torch.nn.Module):\n",
        "    def __init__(self, numclasses):\n",
        "        super(Bert_LSTM, self).__init__()\n",
        "        self.numclasses = numclasses\n",
        "        self.embeddim = embeddim\n",
        "        self.numlayers = numlayers\n",
        "        self.hiddendim_lstm = hiddendim_lstm\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "\n",
        "        #self.lstm = torch.nn.LSTM(self.embeddim, self.hiddendim_lstm*2, bidirectional = True, batch_first=False)\n",
        "        #self.conv = torch.nn.Conv1d( 100, self.hiddendim_lstm, 3, stride=2)\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        print(\"BERT Model Loaded\")\n",
        "        self.lstm = torch.nn.LSTM(self.embeddim, self.hiddendim_lstm, bidirectional = False, batch_first=False)\n",
        "        #self.lstm = torch.nn.GRU(self.embeddim, self.hiddendim_lstm, bidirectional = True, batch_first=False) # noqa\n",
        "        #self.attention = torch.nn.MultiheadAttention(self.hiddendim_lstm*2, num_heads = 2)\n",
        "        self.fc = torch.nn.Linear(self.hiddendim_lstm, 4)\n",
        "        self.fc1 = torch.nn.Linear(self.hiddendim_lstm, 5)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "      last_hidden_state, pooled_output = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "      #print(last_hidden_state.shape)\n",
        "      #print(pooled_output.shape)\n",
        "      out, _ = self.lstm(last_hidden_state, None)\n",
        "      #print(out.shape)\n",
        "      #attn_output, attn_output_weights = self.attention(out, out, out)\n",
        "      #print(attn_output.shape)\n",
        "      #print(attn_output_weights.shape)\n",
        "      NER = self.dropout(out)\n",
        "      ACC = self.dropout(out[:, -1, :])\n",
        "      #print(out.shape)\n",
        "      output = self.fc(NER)\n",
        "      out1 = self.fc1(ACC)\n",
        "      \n",
        "      return output, out1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Bert_GRU(torch.nn.Module):\n",
        "    def __init__(self, numclasses):\n",
        "        super(Bert_GRU, self).__init__()\n",
        "        self.numclasses = numclasses\n",
        "        self.embeddim = embeddim\n",
        "        self.numlayers = numlayers\n",
        "        self.hiddendim_lstm = hiddendim_lstm\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "\n",
        "        #self.lstm = torch.nn.LSTM(self.embeddim, self.hiddendim_lstm*2, bidirectional = True, batch_first=False)\n",
        "        #self.conv = torch.nn.Conv1d( 100, self.hiddendim_lstm, 3, stride=2)\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        print(\"BERT Model Loaded\")\n",
        "        #self.lstm = torch.nn.LSTM(self.embeddim, self.hiddendim_lstm, bidirectional = True, batch_first=False)\n",
        "        self.Gru = torch.nn.GRU(self.embeddim, self.hiddendim_lstm, bidirectional = False, batch_first=False) # noqa\n",
        "        #self.attention = torch.nn.MultiheadAttention(self.hiddendim_lstm*2, num_heads = 2)\n",
        "        self.fc = torch.nn.Linear(self.hiddendim_lstm, 4)\n",
        "        self.fc1 = torch.nn.Linear(self.hiddendim_lstm, 5)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "      last_hidden_state, pooled_output = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "      #print(last_hidden_state.shape)\n",
        "      #print(pooled_output.shape)\n",
        "      out, _ = self.Gru(last_hidden_state, None)\n",
        "      #print(out.shape)\n",
        "      #attn_output, attn_output_weights = self.attention(out, out, out)\n",
        "      #print(attn_output.shape)\n",
        "      #print(attn_output_weights.shape)\n",
        "      NER = self.dropout(out)\n",
        "      ACC = self.dropout(out[:, -1, :])\n",
        "      #print(out.shape)\n",
        "      output = self.fc(NER)\n",
        "      out1 = self.fc1(ACC)\n",
        "      \n",
        "      return output, out1\n",
        "\n",
        "\n",
        "\n",
        "class Bert_Attention(torch.nn.Module):\n",
        "    def __init__(self, numclasses, device):\n",
        "        super(Bert_Attention, self).__init__()\n",
        "        self.numclasses = numclasses\n",
        "        self.embeddim = embeddim\n",
        "        self.numlayers = numlayers\n",
        "        self.hiddendim_lstm = hiddendim_lstm\n",
        "        self.fchidden = fchidden\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        print(\"BERT Model Loaded\")\n",
        "        self.Gru = torch.nn.GRU(self.embeddim, self.hiddendim_lstm, bidirectional = True, batch_first=False)\n",
        "\n",
        "        q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hiddendim_lstm*2))\n",
        "        self.q = torch.nn.Parameter(torch.from_numpy(q_t)).float().to(device)\n",
        "        w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hiddendim_lstm*2, self.fchidden)) # noqa\n",
        "        self.w_h = torch.nn.Parameter(torch.from_numpy(w_ht)).float().to(device)\n",
        "\n",
        "        self.fc = torch.nn.Linear(self.fchidden, 4)\n",
        "        self.fc1 = torch.nn.Linear(self.fchidden, 5)\n",
        "\n",
        "\n",
        "    def forward(self, inp_ids, att_mask, token_ids):\n",
        "        last_hidden_state, pooler_output = self.bert(input_ids=inp_ids, attention_mask=att_mask, token_type_ids=token_ids)\n",
        "        #print(\"1\",last_hidden_state.shape)\n",
        "        gru, _ = self.Gru(last_hidden_state, None)\n",
        "        #print(\"2\",out.shape)\n",
        "        #out = self.attention(out)\n",
        "        att = self.attention(gru)\n",
        "        #print(\"3\",out.shape)\n",
        "        out = self.dropout(att)\n",
        "        print(\"la\",out.shape)\n",
        "        output = self.fc(out)\n",
        "        out1 = self.fc1(out)\n",
        "        #print(\"la\",out.shape)\n",
        "        return output, out1\n",
        "\n",
        "    def attention(self, h):\n",
        "        #print(\"hhhhh\",h.shape)\n",
        "        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n",
        "        v = torch.nn.functional.softmax(v, -1)\n",
        "        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n",
        "        #print(\"tem\",v_temp.shape)\n",
        "        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n",
        "        #print(v.shape)\n",
        "        return v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3T_gqeoDzxu"
      },
      "outputs": [],
      "source": [
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = torch.nn.CrossEntropyLoss()\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)[active_loss]\n",
        "    #active_labels = torch.where(\n",
        "    #    active_loss,\n",
        "    #    target.view(-1),\n",
        "    #    torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    #)\n",
        "    active_labels = target.view(-1)[active_loss]\n",
        "    #print(active_logits.shape)\n",
        "    #print(active_labels.shape)\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss\n",
        "    \n",
        "def loss_fun(outputs, targets):\n",
        "    #print(outputs.shape)\n",
        "    #print(targets.shape)\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN_9uN4XESu3"
      },
      "outputs": [],
      "source": [
        "def evaluate(loader, model, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = 0.0\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        for _,data in enumerate(loader, 0):              \n",
        "            #print(data)\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets_tag = data['target_tag'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "            outputs, out = model(ids, mask, token_type_ids)\n",
        "            loss1 = loss_fn(outputs, targets_tag, mask, num_tag)\n",
        "            loss2 = loss_fun(out, targets)\n",
        "            loss = loss1 + loss2\n",
        "            preds = torch.sigmoid(out)\n",
        "            y_pred.extend(preds.tolist())\n",
        "            y_true.extend(targets.tolist())\n",
        "            y_predict = np.array(y_pred) >= 0.5\n",
        "\n",
        "        F1 = round((metrics.accuracy_score(y_true, y_predict)), 2) * 100\n",
        "        return loss, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YollY3UJEUiy"
      },
      "outputs": [],
      "source": [
        "def train(epoch, model, training_loader, device,optimizer, num_tag):\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets_tag = data['target_tag'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs, out = model(ids, mask, token_type_ids)\n",
        "        #print(outputs.shape)\n",
        "        #print(out.shape)\n",
        "        optimizer.zero_grad()\n",
        "        loss1 = loss_fn(outputs, targets_tag, mask, num_tag)\n",
        "        loss2 = loss_fun(out, targets)\n",
        "        loss = loss1 + loss2\n",
        "        if _%5000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}, Loss1:  {loss1.item()}, Loss2:  {loss2.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlJqwep7ElR6"
      },
      "outputs": [],
      "source": [
        "def validation(epoch, model, device, testing_loader,num_tag):\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_targets_tag=[]\n",
        "    fin_outputs=[]\n",
        "    fin_out=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets_tag = data['target_tag'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs, out = model(ids, mask, token_type_ids)\n",
        "            loss1 = loss_fn(outputs, targets_tag, mask, num_tag)\n",
        "            loss2 = loss_fun(out, targets)\n",
        "            loss = loss1 + loss2\n",
        "            if _%5000==0:\n",
        "                print(f'Epoch: {epoch}, Loss:  {loss.item()}, Loss1:  {loss1.item()}, Loss2:  {loss2.item()}')\n",
        "            #outputs = torch.log(outputs)\n",
        "            #outputs = torch.cat(outputs, dim=1).flatten()\n",
        "            #print(outputs)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_targets_tag.extend(targets_tag.cpu().detach().numpy().tolist())\n",
        "            fin_out.extend(torch.sigmoid(out).cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets, fin_targets_tag, fin_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0NYIdbnFFvQ"
      },
      "outputs": [],
      "source": [
        "def validate(EPOCHS,model, device, testing_loader,num_tag):\n",
        "  best_accuracy = 0\n",
        "  predictions = []\n",
        "  truetarget = []\n",
        "  for epoch in range(EPOCHS):\n",
        "      outputs, targets, targets_tag, out = validation(epoch, model, device, testing_loader,num_tag)\n",
        "      print(out[1])\n",
        "      print(targets[1])\n",
        "      for p, r in zip(outputs, targets_tag):\n",
        "        #print(p)\n",
        "        #print(r)\n",
        "        outs = []\n",
        "        tar = []\n",
        "        for m, n in zip (p,r):\n",
        "          if (n != 0):\n",
        "            a = m.index(max(m))\n",
        "            #print(a)\n",
        "            tar.append(n)\n",
        "            outs.append(a)\n",
        "            #print(outs)\n",
        "        predictions.append(outs)\n",
        "        truetarget.append(tar)\n",
        "          \n",
        "      print(predictions[1])\n",
        "      print(truetarget[1])\n",
        "      print(outputs[1])\n",
        "      print(targets_tag[1]) \n",
        "      out = np.array(out) >= 0.5\n",
        "      #accuracy = metrics.accuracy_score(targets_tag, outputs)\n",
        "      accuracy = metrics.accuracy_score(targets, out)\n",
        "      f1_score_micro = metrics.f1_score(targets, out, average='micro')\n",
        "      f1_score_macro = metrics.f1_score(targets, out, average='macro')\n",
        "      #precision_score_micro = metrics.precision_score(targets, outputs, average='micro')\n",
        "      precision_score_macro = metrics.precision_score(targets, out, average='macro')\n",
        "      #recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
        "      recall_score_macro = metrics.recall_score(targets, out, average='macro')\n",
        "      print(f\"Accuracy Score = {accuracy}\")\n",
        "      print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "      print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
        "      #print(f\"precision_score (Micro) = {precision_score_micro}\")\n",
        "      print(f\"precision_score (Macro) = {precision_score_macro}\")\n",
        "      #print(f\"recall_score (Micro) = {recall_score_micro}\")\n",
        "      print(f\"recall_score (Macro) = {recall_score_macro}\")\n",
        "      if accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "      return predictions, truetarget, out, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gLtiNuvGJpl"
      },
      "outputs": [],
      "source": [
        "def report(predictions, truetarget):\n",
        "  #print(\"F1-Score: {}\".format(f1_score(pred, targ)))\n",
        "  print(\"Validation Accuracy: {}\".format(accuracy_score(predictions, truetarget)))\n",
        "  pred = []\n",
        "  targ = []\n",
        "  for p in predictions:\n",
        "    #print(p)\n",
        "    q = []\n",
        "    for p_i in p:\n",
        "      #print(p_i)\n",
        "      if p_i == 1:\n",
        "        q.append('O')\n",
        "        #print(p_i)\n",
        "      elif p_i == 2:\n",
        "        q.append('B')\n",
        "        #print(p_i)\n",
        "      else:\n",
        "        q.append('I')\n",
        "    pred.append(q)\n",
        "      #print(predictions)\n",
        "      #print(pred)\n",
        "  for p in truetarget:\n",
        "    #print(p)\n",
        "    q = []\n",
        "    for p_i in p:\n",
        "      #print(p_i)\n",
        "      if p_i == 1:\n",
        "        q.append('O')\n",
        "        #print(p_i)\n",
        "      elif p_i == 2:\n",
        "        q.append('B')\n",
        "        #print(p_i)\n",
        "      else:\n",
        "        q.append('I')\n",
        "    targ.append(q)\n",
        "  print(pred)\n",
        "  print(targ)\n",
        "  class_names = ['Negative', 'Neutral', 'Positive']\n",
        "  print(\"F1-Score: {}\".format(classification_report(pred, targ)))\n",
        "  print(\"F1-Score: {}\".format(f1_score(pred, targ)))\n",
        "\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(accuracy_score(pred, targ)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nyQp71QZBqL_",
        "outputId": "8e588cfb-1cfe-4e1c-a341-cf60412d50ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirements: \n",
            "numpy  1.21.6\n",
            "pandas  1.3.5\n",
            "sklearn  1.0.2\n",
            "torch  1.10.0+cu111\n",
            "transformers 4.18.0\n",
            "-----------------------------------------------------\n",
            "Device =  cuda\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "-----------------------------------------------------\n",
            "                                                text             list\n",
            "0                    The bread is top notch as well.  [0, 1, 0, 0, 0]\n",
            "1  I have to say they have one of the fastest del...  [1, 0, 0, 0, 0]\n",
            "2        Food is always fresh and hot- ready to eat!  [0, 1, 0, 0, 0]\n",
            "3      Did I mention that the coffee is OUTSTANDING?  [0, 1, 0, 0, 0]\n",
            "4  Certainly not the best sushi in New York, howe...  [0, 1, 0, 0, 1]\n",
            "-----------------------------------------------------\n",
            "                                           Sentence1  \\\n",
            "0                     the bread is top notch as well   \n",
            "1  i have to say they have one of the fastest del...   \n",
            "2       food is always fresh and hot- ready to eat !   \n",
            "3       did i mention that the coffee is outstanding   \n",
            "4  certainly not the best sushi in new york , how...   \n",
            "\n",
            "                                           target1  \n",
            "0                                    0 1 0 0 0 0 0  \n",
            "1                    0 0 0 0 0 0 0 0 0 0 1 2 0 0 0  \n",
            "2                              1 0 0 0 0 0 0 0 0 0  \n",
            "3                                  0 0 0 0 0 1 0 0  \n",
            "4  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0  \n",
            "-----------------------------------------------------\n",
            "   Sentence   Word target\n",
            "0         1    the      0\n",
            "1         1  bread      1\n",
            "2         1     is      0\n",
            "3         1    top      0\n",
            "4         1  notch      0\n",
            "-----------------------------------------------------\n",
            "   Sentence   Word target\n",
            "0         1    the      O\n",
            "1         1  bread      B\n",
            "2         1     is      O\n",
            "3         1    top      O\n",
            "4         1  notch      O\n",
            "-----------------------------------------------------\n",
            "   Sentence   Word  target\n",
            "0         1    the       1\n",
            "1         1  bread       2\n",
            "2         1     is       1\n",
            "3         1    top       1\n",
            "4         1  notch       1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------\n",
            "                                                text             list\n",
            "0                but the staff was so horrible to us  [1, 0, 0, 0, 0]\n",
            "1  to be completely fair the only redeeming facto...  [0, 1, 1, 0, 0]\n",
            "2  the food is uniformly exceptional with a very ...  [0, 1, 0, 0, 0]\n",
            "3  where gabriela personaly greets you and recomm...  [1, 0, 0, 0, 0]\n",
            "4  for those that go once and dont enjoy it all i...  [0, 0, 1, 0, 0]\n",
            "-----------------------------------------------------\n",
            "                                           Sentence1  \\\n",
            "0                but the staff was so horrible to us   \n",
            "1  to be completely fair , the only redeeming fac...   \n",
            "2  the food is uniformly exceptional , with a ver...   \n",
            "3  where gabriela personaly greets you and recomm...   \n",
            "4  for those that go once and do n't enjoy it , a...   \n",
            "\n",
            "                                             target1  \n",
            "0                                    0 0 1 0 0 0 0 0  \n",
            "1  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
            "2  0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
            "3                              0 0 0 0 0 0 0 0 0 0 0  \n",
            "4      0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  \n",
            "-----------------------------------------------------\n",
            "   Sentence   Word target\n",
            "0         1    but      0\n",
            "1         1    the      0\n",
            "2         1  staff      1\n",
            "3         1    was      0\n",
            "4         1     so      0\n",
            "-----------------------------------------------------\n",
            "   Sentence   Word target\n",
            "0         1    but      O\n",
            "1         1    the      O\n",
            "2         1  staff      B\n",
            "3         1    was      O\n",
            "4         1     so      O\n",
            "-----------------------------------------------------\n",
            "   Sentence   Word  target\n",
            "0         1    but       1\n",
            "1         1    the       1\n",
            "2         1  staff       2\n",
            "3         1    was       1\n",
            "4         1     so       1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------\n",
            "                                                temp  \\\n",
            "0             [the, bread, is, top, notch, as, well]   \n",
            "1  [i, have, to, say, they, have, one, of, the, f...   \n",
            "2  [food, is, always, fresh, and, hot-, ready, to...   \n",
            "3  [did, i, mention, that, the, coffee, is, outst...   \n",
            "4  [certainly, not, the, best, sushi, in, new, yo...   \n",
            "\n",
            "                                              target  \\\n",
            "0                              [1, 2, 1, 1, 1, 1, 1]   \n",
            "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1]   \n",
            "2                     [2, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
            "3                           [1, 1, 1, 1, 1, 2, 1, 1]   \n",
            "4  [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "\n",
            "                                                text             list  \n",
            "0                    The bread is top notch as well.  [0, 1, 0, 0, 0]  \n",
            "1  I have to say they have one of the fastest del...  [1, 0, 0, 0, 0]  \n",
            "2        Food is always fresh and hot- ready to eat!  [0, 1, 0, 0, 0]  \n",
            "3      Did I mention that the coffee is OUTSTANDING?  [0, 1, 0, 0, 0]  \n",
            "4  Certainly not the best sushi in New York, howe...  [0, 1, 0, 0, 1]  \n",
            "-----------------------------------------------------\n",
            "                                                temp  \\\n",
            "0       [but, the, staff, was, so, horrible, to, us]   \n",
            "1  [to, be, completely, fair, ,, the, only, redee...   \n",
            "2  [the, food, is, uniformly, exceptional, ,, wit...   \n",
            "3  [where, gabriela, personaly, greets, you, and,...   \n",
            "4  [for, those, that, go, once, and, do, n't, enj...   \n",
            "\n",
            "                                              target  \\\n",
            "0                           [1, 1, 2, 1, 1, 1, 1, 1]   \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, ...   \n",
            "2  [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, ...   \n",
            "3                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
            "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "\n",
            "                                                text             list  \n",
            "0                but the staff was so horrible to us  [1, 0, 0, 0, 0]  \n",
            "1  to be completely fair the only redeeming facto...  [0, 1, 1, 0, 0]  \n",
            "2  the food is uniformly exceptional with a very ...  [0, 1, 0, 0, 0]  \n",
            "3  where gabriela personaly greets you and recomm...  [1, 0, 0, 0, 0]  \n",
            "4  for those that go once and dont enjoy it all i...  [0, 0, 1, 0, 0]  \n",
            "TRAIN Dataset: (3044, 4)\n",
            "TEST Dataset: (800, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------\n",
            "-----------------------------------------------------\n",
            "Epoch: 0, Loss:  2.2425453662872314, Loss1:  1.4859684705734253, Loss2:  0.7565769553184509\n",
            "Epoch: 1, Loss:  0.4124625325202942, Loss1:  0.07674767822027206, Loss2:  0.33571484684944153\n",
            "Epoch: 2, Loss:  0.12883272767066956, Loss1:  0.03634382411837578, Loss2:  0.09248889982700348\n",
            "Epoch: 3, Loss:  0.18832442164421082, Loss1:  0.059120092540979385, Loss2:  0.12920433282852173\n",
            "-----------------------------------------------------\n",
            "Epoch: 0, Loss:  0.15201054513454437, Loss1:  0.1351136416196823, Loss2:  0.01689690724015236\n",
            "[0.010009605437517166, 0.977443516254425, 0.05494960770010948, 0.009500931017100811, 0.008510638028383255]\n",
            "[0.0, 1.0, 0.0, 0.0, 0.0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[[0.04280104860663414, 0.9916508793830872, 0.1403278261423111, 0.26926183700561523], [0.05199909582734108, 0.9990639090538025, 0.029477927833795547, 0.09378097206354141], [0.05049366131424904, 0.9991996884346008, 0.03695773333311081, 0.09495847672224045], [0.05326573923230171, 0.9989957213401794, 0.036075808107852936, 0.08675699681043625], [0.04126592352986336, 0.9988791346549988, 0.04390905052423477, 0.10172679275274277], [0.0480780228972435, 0.9991267323493958, 0.04399362578988075, 0.0851486548781395], [0.016082964837551117, 0.990346372127533, 0.5256639122962952, 0.16320545971393585], [0.054281990975141525, 0.25024643540382385, 0.9833630323410034, 0.6721644401550293], [0.04043467342853546, 0.9988424181938171, 0.0322159081697464, 0.15592731535434723], [0.05112946778535843, 0.9991496801376343, 0.0400107316672802, 0.08497091382741928], [0.02991286851465702, 0.9978164434432983, 0.11598410457372665, 0.1436026394367218], [0.031184906139969826, 0.8246505260467529, 0.973373293876648, 0.1817188560962677], [0.048829566687345505, 0.9992037415504456, 0.038039010018110275, 0.09061476588249207], [0.0297922994941473, 0.9427167177200317, 0.8803094029426575, 0.2063213437795639], [0.03711263835430145, 0.9897088408470154, 0.11835058778524399, 0.509711503982544], [0.028112679719924927, 0.9326751232147217, 0.5360100269317627, 0.6094786524772644], [0.030038274824619293, 0.9906846284866333, 0.14120328426361084, 0.4931865334510803], [0.029607048258185387, 0.9931777715682983, 0.16141270101070404, 0.3822694420814514], [0.04822336509823799, 0.9991163611412048, 0.031467363238334656, 0.08693531900644302], [0.053021177649497986, 0.999097466468811, 0.03347977250814438, 0.08158627152442932], [0.055462270975112915, 0.9988077878952026, 0.028593147173523903, 0.12368953227996826], [0.052199460566043854, 0.998892605304718, 0.03410310298204422, 0.0886349305510521], [0.037033092230558395, 0.9978772401809692, 0.0792609229683876, 0.1288572996854782], [0.048684973269701004, 0.9977608919143677, 0.05149294063448906, 0.14165472984313965], [0.037400197237730026, 0.9970920085906982, 0.12131047993898392, 0.10893306136131287], [0.03546442836523056, 0.9962913990020752, 0.14478814601898193, 0.1303681880235672], [0.04706541448831558, 0.9970743656158447, 0.17105932533740997, 0.09355214983224869], [0.03342264145612717, 0.9965206384658813, 0.12745890021324158, 0.1358933448791504], [0.03555290028452873, 0.9964687824249268, 0.1607092022895813, 0.13230714201927185], [0.044897016137838364, 0.9960720539093018, 0.18665432929992676, 0.09959054738283157], [0.06057664379477501, 0.9779466390609741, 0.49007293581962585, 0.2254941463470459], [0.053396351635456085, 0.9927520751953125, 0.283123254776001, 0.12095217406749725], [0.06673402339220047, 0.9478340148925781, 0.6483898758888245, 0.2602385878562927], [0.05657548829913139, 0.9487955570220947, 0.6920179128646851, 0.24830473959445953], [0.07715193182229996, 0.8885073661804199, 0.8061828017234802, 0.2839174270629883], [0.06926512718200684, 0.9750399589538574, 0.5059255361557007, 0.21661020815372467], [0.043072085827589035, 0.9790152311325073, 0.5036939978599548, 0.21941445767879486], [0.1127985492348671, 0.700903058052063, 0.8920323252677917, 0.4543569087982178], [0.06960871070623398, 0.9611409902572632, 0.5538098812103271, 0.23645739257335663], [0.07656952738761902, 0.9566900134086609, 0.5499603748321533, 0.24332471191883087], [0.12050943821668625, 0.8184279799461365, 0.8304835557937622, 0.3733181357383728], [0.07279862463474274, 0.9320231080055237, 0.5315755009651184, 0.45202958583831787], [0.10114457458257675, 0.9231006503105164, 0.6271682381629944, 0.3050728738307953], [0.1147349625825882, 0.7112884521484375, 0.5946443676948547, 0.6866603493690491], [0.07574805617332458, 0.8910821080207825, 0.4281157851219177, 0.6585081815719604], [0.07216440141201019, 0.8706728219985962, 0.44588568806648254, 0.6663967967033386], [0.054003458470106125, 0.9969791173934937, 0.13104210793972015, 0.09855500608682632], [0.05131113901734352, 0.9930603504180908, 0.22061125934123993, 0.11572983860969543], [0.051445890218019485, 0.9954648613929749, 0.17569506168365479, 0.09423401206731796], [0.03647360950708389, 0.9964368343353271, 0.1783907562494278, 0.0978919267654419], [0.03270597755908966, 0.9927120208740234, 0.26891815662384033, 0.15068446099758148], [0.055227234959602356, 0.9853193759918213, 0.3924059569835663, 0.14828677475452423], [0.03215859830379486, 0.9958285689353943, 0.15453305840492249, 0.14579826593399048], [0.040963269770145416, 0.9932976365089417, 0.2875204384326935, 0.15760086476802826], [0.04525395855307579, 0.9972038269042969, 0.1487296223640442, 0.08205394446849823], [0.0483814962208271, 0.9972617626190186, 0.15767477452754974, 0.08169909566640854], [0.04822976514697075, 0.9970876574516296, 0.1613920032978058, 0.08456826955080032], [0.04725106433033943, 0.9972501397132874, 0.16817156970500946, 0.08746620267629623], [0.05335839465260506, 0.996263325214386, 0.21724878251552582, 0.09194006025791168], [0.06381446868181229, 0.9818399548530579, 0.44351479411125183, 0.20393086969852448], [0.06325440853834152, 0.9612929821014404, 0.5874627232551575, 0.2540428638458252], [0.07273253053426743, 0.777432382106781, 0.8353753685951233, 0.4000726342201233], [0.142147034406662, 0.43219947814941406, 0.6923947334289551, 0.8719930648803711], [0.0665840357542038, 0.9831022620201111, 0.4306924045085907, 0.1960485428571701], [0.07208231091499329, 0.9756986498832703, 0.4751664102077484, 0.23483602702617645], [0.07472901046276093, 0.9539602994918823, 0.5829313397407532, 0.2665826082229614], [0.11762170493602753, 0.6345764398574829, 0.8679042458534241, 0.5967752933502197], [0.06036573648452759, 0.979598879814148, 0.35132187604904175, 0.30470550060272217], [0.12731021642684937, 0.7914974093437195, 0.8379462361335754, 0.3675593435764313], [0.09835412353277206, 0.9244970679283142, 0.6462417840957642, 0.28535351157188416], [0.11303526908159256, 0.8091588616371155, 0.5440457463264465, 0.6170000433921814], [0.037700653076171875, 0.9804651737213135, 0.203885018825531, 0.5089255571365356], [0.07907889038324356, 0.8293817043304443, 0.43372976779937744, 0.7543195486068726], [0.04712992534041405, 0.9950423240661621, 0.18343031406402588, 0.09313997626304626], [0.05041877180337906, 0.9950390458106995, 0.1994754672050476, 0.09864867478609085], [0.060782209038734436, 0.9950219392776489, 0.1760707050561905, 0.12902048230171204], [0.044110823422670364, 0.9957846999168396, 0.18851564824581146, 0.09920784085988998], [0.031875260174274445, 0.9962989687919617, 0.13519570231437683, 0.12774619460105896], [0.03187461942434311, 0.9956284761428833, 0.17217612266540527, 0.13035936653614044], [0.032828543335199356, 0.9960662722587585, 0.1451766937971115, 0.1303589791059494], [0.0323525033891201, 0.9958076477050781, 0.15389403700828552, 0.13862301409244537], [0.0332343764603138, 0.9953704476356506, 0.16854554414749146, 0.14461937546730042], [0.03334864228963852, 0.9968390464782715, 0.1172645166516304, 0.12744873762130737], [0.04206796735525131, 0.9877190589904785, 0.3613828718662262, 0.2093656063079834], [0.04220502823591232, 0.9962713718414307, 0.19595637917518616, 0.11549822986125946], [0.04883016273379326, 0.9964371919631958, 0.17543211579322815, 0.09444466233253479], [0.049226243048906326, 0.9955839514732361, 0.21346279978752136, 0.10304991900920868], [0.05181782692670822, 0.9954427480697632, 0.23799827694892883, 0.12410438805818558], [0.05791686475276947, 0.9886828064918518, 0.33159831166267395, 0.18576651811599731], [0.06540228426456451, 0.9535946249961853, 0.616230309009552, 0.27131760120391846], [0.07520521432161331, 0.7918145060539246, 0.8191372156143188, 0.3899479806423187], [0.13004839420318604, 0.5052396655082703, 0.7504668831825256, 0.8009721040725708], [0.06627216190099716, 0.9835999011993408, 0.4075869917869568, 0.2054448276758194], [0.06782954931259155, 0.9810989499092102, 0.41913408041000366, 0.21560195088386536], [0.07212701439857483, 0.9693877100944519, 0.495675265789032, 0.24079535901546478], [0.11195941269397736, 0.6660877466201782, 0.8584318161010742, 0.589174747467041], [0.05191998556256294, 0.9732037782669067, 0.3726062476634979, 0.40773719549179077], [0.11064093559980392, 0.8857271671295166, 0.7625191807746887, 0.29660657048225403], [0.13339251279830933, 0.7799198627471924, 0.8202241063117981, 0.3775334656238556], [0.1075795367360115, 0.886079728603363, 0.6598719954490662, 0.35477980971336365]]\n",
            "[1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Accuracy Score = 0.82375\n",
            "F1 Score (Micro) = 0.9047619047619048\n",
            "F1 Score (Macro) = 0.8948128899455969\n",
            "precision_score (Macro) = 0.9368317323119187\n",
            "recall_score (Macro) = 0.8575974447558703\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-2525bcde9dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-2525bcde9dd1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'service'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'food'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'anecdotes/miscellaneous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'price'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ambiance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, digits, suffix, output_dict, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    677\u001b[0m                   )\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m     \u001b[0mtarget_names_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m     \u001b[0mtarget_names_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names_true\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtarget_names_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mget_entities\u001b[0;34m(seq, suffix)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mbegin_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0m_validate_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('bool'), dtype('<U1')) -> None"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \n",
        "    print(\"Requirements: \")\n",
        "    requirements()\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n",
        "\n",
        "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "    print(\"Device = \", device)\n",
        "    drive.mount('/content/drive/')\n",
        "    datapath= '/content/drive/MyDrive/Bert_mtl/Data/Data_For_ATE_and_ACD/'\n",
        "\n",
        "    data_test_category = datapath + 'Test/MTLBACategory.csv'\n",
        "    data_test_sentense = datapath + 'Test/sentence (1).txt'\n",
        "    data_test_target = datapath + 'Test/target (1).txt'\n",
        "\n",
        "    data_train_category = datapath + 'Train/semeval2014.csv'\n",
        "    data_train_sentense = datapath + 'Train/sentence.txt'\n",
        "    data_train_target = datapath + 'Train/target.txt'\n",
        "\n",
        "    df_test = data_loader(data_test_category, data_test_sentense, data_test_target)\n",
        "    df_train = data_loader(data_train_category, data_train_sentense, data_train_target)\n",
        "\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(df_test.head(5))\n",
        "\n",
        "\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(df_train.head(5))\n",
        "\n",
        "    # Sections of config\n",
        "\n",
        "    # Defining some key variables that will be used later on in the training\n",
        "    MAX_LEN = 100\n",
        "    TRAIN_BATCH_SIZE = 16\n",
        "    VALID_BATCH_SIZE = 4\n",
        "    EPOCHS = 4\n",
        "    LEARNING_RATE = 2e-05\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',return_dict=False)\n",
        "\n",
        "\n",
        "    train_size = 1.00\n",
        "    test_size = 1.0\n",
        "    train_dataset=df_train.sample(frac=train_size,random_state=200)\n",
        "    test_dataset=df_test.sample(frac=test_size,random_state=200)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "    testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "\n",
        "    train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    testing_loader = DataLoader(testing_set, **test_params)\n",
        "\n",
        "\n",
        "    fchidden = 256\n",
        "    hiddendim_lstm = 256\n",
        "    embeddim = 768\n",
        "    numlayers = 12\n",
        "    numclasses = 4\n",
        "    num_heads = 2\n",
        "\n",
        "    model = BERTClass(numclasses,numlayers,num_heads, embeddim , hiddendim_lstm, fchidden)\n",
        "    model.to(device)\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    model\n",
        "\n",
        "    print(\"-----------------------------------------------------\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "    import joblib\n",
        "\n",
        "    #meta_data = {\n",
        "    #    \"enc_tag\": enc_tag\n",
        "    #}\n",
        "\n",
        "    #joblib.dump(meta_data, \"meta.bin\")\n",
        "\n",
        "    #num_tag = len(list(enc_tag.classes_))\n",
        "    num_tag = 4\n",
        "    \n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "      train(epoch, model, training_loader,device,optimizer,num_tag)\n",
        "\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    predictions, truetarget, out, targets = validate(EPOCHS, model,device, testing_loader,num_tag)\n",
        "\n",
        "\n",
        "    target_names = ['service', 'food', 'anecdotes/miscellaneous', 'price', 'ambiance']\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(classification_report(out, targets, target_names))\n",
        "\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    #report(predictions, truetarget)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7RxVsSzSV6u"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "bert_atl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}